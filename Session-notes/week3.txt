								**********************************************************

											  Session 1

								**********************************************************

Talk on Modularization (Reviewing)
	(Question Answering + Discssion + Add points)
- resuability
- scalability of our app/websit

init -> Initailization of packages

Ak folder ki chezain dusary folder main use karny kayliy
----------------------------
Uni-Testing: a approach to test your code on different cases for make it better.
- what: testing framework
- where: python project
- importance: efficiency
- ease: readability
- power: flexitbility
- advantage: scalability
what are major libraries of Uni-testing in python?
1-unittest:
	- build in python library
	- inspired by the Junit framword
	- follow the xUnit sytle of setup, excution, andtreadwon
2-pytest: better than 
3-nose
4-doctest
5-nose2
6-hypothesis
7-tox
8-unitest2
9-testtools
10-pyvows

mostly used pytest.

what are application
 web develop, data science, ml, devops, game development, embedded system, desktop application.

------------------

req.txt
- pytest install

asset keyword - kisi function kay result kay uper True/False baty ga.

--------------------------
Sir aqibRehman shared a file
----------------------------
see the file -> week3_sir_AqibRehman-file.ipynb

======================================================================================================================================================
						*************************************************
               
								Session 2

						*************************************************

Topics: end-to-end projects stages
- stages discuss
- tools in stages
                   
1. Data Collection & preprocessing
	data from various sources, cleaned, preprocessing
	collection-source:
	  - Web scraping: beautifulsoup, scrapy
	  - APis: urllib, postman
	  - streamming data: apache, kafka, apache flink

preprocessing
cleaning data: Pandas, dask
features eng: features-engin, featurestools
data-transformation: scikit-learn, tensorflow and scipy

EDA stages 
tools: data visualization: matplot, seaborn

model building & training
tools
ML: tensorflow, pytorch, scikit-learn, XGBoost, lightGBM
AutoML
Auto-sklearn, H20.ai, Google Cloud autoML
Hyperparameter tuning:
 hyperopt, optuna, scikit-optimize
Experiment Tracking
 MLflow, weights & Biases, Comet.ml

Model Evaluation & validation 
Evaluation: KPIs defined, performance indicators defined(mean square error etc) by doing this we know how our model is performing

discussion: accuracy sai ni hai to hyperparameter ki ak technique lagty hain

Tools: 
	metrices calaulation: scikit-learn, tensorflow model analysis
 	model-interpretability: SHAP(explainable AI), line, eli5

Model deployment:
tools: 
	Flask,fastAPI, 
	Jab model depoly hogya tabhi
	(in Docker)Cantainerization: VM - gave some resources from acutal machine to vm machine
			  2 waja say concept aya: apni app ko kisi or aja deplpoy karna hai, resources scalability
		jb ap apni file/model kisi ko testing kay liy dethy hain to agr unki machine apky model ko support na kary to VM use hota tha lakin
			ap us file ko contarization karty hain.	
				
		Docker, Kubernetes
	Orchestration: kubernetes, apache ariflow

======================================================================================================================================================
						*************************************************
               
								Session 3

						*************************************************

Docker Engine: 
	
	- file give to tester: when u'r tester person don't have such good or such those files in their pc then the docker comes
	
	must present the docker engine
		a project have different folders/files present in git
	how a docker image create, kisi or ki machine main apka project run karny kaly apky project ki requirment es main hoti hai
	
	- docker file have config/requiremts
	- image => whole depandancies integration but it's not in running state of application. just a package.	
	- image running state is called `container`.
	- Kubernetes - high availability, scalability performance. disaater recovery
		overall all architecture
			
		cluster are the combination of nodes which are connect each other.
			normally there a 2 master nodes for recovery and remaining are worker nodes.

			one is master node and multiple workers node
				each worker node run service which is called kubelet.
			Node? this is physical/virtual machine
			On each worker node container are run

Compoents of master Node

  - APIs server(UI configuration,Api,CLI) 
  - controller manager
  - Scheduler

POds

Sir AqibRehman PirZada share screen and explain some points..